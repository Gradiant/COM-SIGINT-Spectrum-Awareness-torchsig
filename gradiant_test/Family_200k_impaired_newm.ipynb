{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3853186c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Family Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147541a",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to train our new model (ResNet1-size?) in order to classify signal family from impaired data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc87d51",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60290c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchsig.transforms as ST\n",
    "import numpy as np\n",
    "import torchsig\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.data import Subset\n",
    "from sigfam import Sig53\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from Train_evaluate import *\n",
    "from modeling import *\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae11af4",
   "metadata": {},
   "source": [
    "----\n",
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed12a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [\n",
    "    \"psk\",\n",
    "    \"qam\",\n",
    "    \"ofdm\"\n",
    "    ]\n",
    "\n",
    "modulation_mapping = {\n",
    "    \"psk\": 0,\n",
    "    \"qam\": 1,\n",
    "    \"ofdm\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487a9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_family_dict = {\n",
    "        \"bpsk\": \"psk\",\n",
    "        \"qpsk\": \"psk\",\n",
    "        \"8psk\": \"psk\",\n",
    "        \"16psk\": \"psk\",\n",
    "        \"32psk\": \"psk\",\n",
    "        \"64psk\": \"psk\",\n",
    "        \"16qam\": \"qam\",\n",
    "        \"32qam\": \"qam\",\n",
    "        \"32qam_cross\": \"qam\",\n",
    "        \"64qam\": \"qam\",\n",
    "        \"128qam_cross\": \"qam\",\n",
    "        \"256qam\": \"qam\",\n",
    "        \"512qam_cross\": \"qam\",\n",
    "        \"1024qam\": \"qam\",\n",
    "        \"ofdm-64\": \"ofdm\",\n",
    "        \"ofdm-72\": \"ofdm\",\n",
    "        \"ofdm-128\": \"ofdm\",\n",
    "        \"ofdm-180\": \"ofdm\",\n",
    "        \"ofdm-256\": \"ofdm\",\n",
    "        \"ofdm-300\": \"ofdm\",\n",
    "        \"ofdm-512\": \"ofdm\",\n",
    "        \"ofdm-600\": \"ofdm\",\n",
    "        \"ofdm-900\": \"ofdm\",\n",
    "        \"ofdm-1024\": \"ofdm\",\n",
    "        \"ofdm-1200\": \"ofdm\",\n",
    "        \"ofdm-2048\": \"ofdm\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9f7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 200000\n",
      "Data shape: (2, 4096)\n",
      "Label Index: 18\n",
      "Label Class: 32qam\n"
     ]
    }
   ],
   "source": [
    "# Specify Sig53 Options\n",
    "root = \"../../../../data/torchsig/sigfam/\"\n",
    "train = True\n",
    "impaired = True\n",
    "class_list = list(Sig53._idx_to_name_dict.values())\n",
    "transform = ST.Compose([\n",
    "    ST.RandomPhaseShift(phase_offset=(-1, 1)),\n",
    "    ST.Normalize(norm=np.inf),\n",
    "    ST.ComplexTo2D(),\n",
    "])\n",
    "target_transform = ST.DescToClassIndex(class_list=class_list)\n",
    "\n",
    "# Instantiate the Sig53 Clean Training Dataset\n",
    "sig53_clean_train = Sig53(\n",
    "    root=root,\n",
    "    train=train,\n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    use_signal_data=True,\n",
    ")\n",
    "\n",
    "# Instantiate the Sig53 Clean Validation Dataset\n",
    "\n",
    "train = False\n",
    "sig53_clean_val = Sig53(\n",
    "    root=root,\n",
    "    train=train,\n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    use_signal_data=True,\n",
    ")\n",
    "\n",
    "# Retrieve a sample and print out information to verify\n",
    "idx = np.random.randint(len(sig53_clean_train))\n",
    "data, label = sig53_clean_train[idx]\n",
    "print(\"Dataset length: {}\".format(len(sig53_clean_train)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "print(\"Label Index: {}\".format(label))\n",
    "print(\"Label Class: {}\".format(Sig53.convert_idx_to_name(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6951416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset length: 200000\n",
      "First sample data shape: (2, 4096)\n",
      "First sample family name: psk\n",
      "First sample family index: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold the new dataset\n",
    "family_train_dataset = []\n",
    "\n",
    "# Iterate over the training dataset\n",
    "for i in range(len(sig53_clean_train)):\n",
    "    # Retrieve data and label for the current index\n",
    "    data, label = sig53_clean_train[i]\n",
    "\n",
    "    # Get the class name from the label index\n",
    "    class_name = Sig53.convert_idx_to_name(label)\n",
    "\n",
    "    # Map the class name to the family name\n",
    "    family_name = class_family_dict[class_name]\n",
    "\n",
    "    # Convert the family name to its corresponding index\n",
    "    family_index = modulation_mapping[family_name]\n",
    "\n",
    "    # Append the data and the family index to the new dataset\n",
    "    family_train_dataset.append((data, family_index))\n",
    "\n",
    "# Verify the new dataset\n",
    "print(\"New dataset length: {}\".format(len(family_train_dataset)))\n",
    "print(\"First sample data shape: {}\".format(family_train_dataset[0][0].shape))\n",
    "\n",
    "# Retrieve the family name using the index\n",
    "first_sample_family_index = family_train_dataset[0][1]\n",
    "family_name_by_index = {v: k for k, v in modulation_mapping.items()}[first_sample_family_index]\n",
    "\n",
    "print(\"First sample family name: {}\".format(family_name_by_index))\n",
    "print(\"First sample family index: {}\".format(first_sample_family_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae5d69",
   "metadata": {},
   "source": [
    "----\n",
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf06854-b22f-4269-8661-e9cab52b39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of the validation set (10% of the training set)\n",
    "val_size = int(0.1 * len(sig53_clean_train))\n",
    "train_size = len(sig53_clean_train) - val_size\n",
    "\n",
    "# Split the training dataset into training and validation datasets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(family_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=sig53_clean_val,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523b85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, _ = next(iter(train_dataloader))\n",
    "input_size = inputs.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6968a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNet1D(Bottleneck1D, [3,4,4,3], num_classes=len(selected_classes), in_channels=2).to(device)\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ceab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 32, 2048]             448\n",
      "       BatchNorm1d-2             [-1, 32, 2048]              64\n",
      "            Conv1d-3             [-1, 32, 2048]           1,024\n",
      "       BatchNorm1d-4             [-1, 32, 2048]              64\n",
      "            Conv1d-5             [-1, 32, 2048]              96\n",
      "            Conv1d-6             [-1, 32, 2048]           1,024\n",
      "DepthwiseSeparableConv1d-7             [-1, 32, 2048]               0\n",
      "       BatchNorm1d-8             [-1, 32, 2048]              64\n",
      "            Conv1d-9            [-1, 128, 2048]           4,096\n",
      "      BatchNorm1d-10            [-1, 128, 2048]             256\n",
      "AdaptiveAvgPool1d-11               [-1, 128, 1]               0\n",
      "           Linear-12                    [-1, 8]           1,024\n",
      "             ReLU-13                    [-1, 8]               0\n",
      "           Linear-14                  [-1, 128]           1,024\n",
      "          Sigmoid-15                  [-1, 128]               0\n",
      "          SEBlock-16            [-1, 128, 2048]               0\n",
      "           Conv1d-17            [-1, 128, 2048]           4,096\n",
      "      BatchNorm1d-18            [-1, 128, 2048]             256\n",
      "     Bottleneck1D-19            [-1, 128, 2048]               0\n",
      "           Conv1d-20             [-1, 32, 2048]           4,096\n",
      "      BatchNorm1d-21             [-1, 32, 2048]              64\n",
      "           Conv1d-22             [-1, 32, 2048]              96\n",
      "           Conv1d-23             [-1, 32, 2048]           1,024\n",
      "DepthwiseSeparableConv1d-24             [-1, 32, 2048]               0\n",
      "      BatchNorm1d-25             [-1, 32, 2048]              64\n",
      "           Conv1d-26            [-1, 128, 2048]           4,096\n",
      "      BatchNorm1d-27            [-1, 128, 2048]             256\n",
      "AdaptiveAvgPool1d-28               [-1, 128, 1]               0\n",
      "           Linear-29                    [-1, 8]           1,024\n",
      "             ReLU-30                    [-1, 8]               0\n",
      "           Linear-31                  [-1, 128]           1,024\n",
      "          Sigmoid-32                  [-1, 128]               0\n",
      "          SEBlock-33            [-1, 128, 2048]               0\n",
      "     Bottleneck1D-34            [-1, 128, 2048]               0\n",
      "           Conv1d-35             [-1, 32, 2048]           4,096\n",
      "      BatchNorm1d-36             [-1, 32, 2048]              64\n",
      "           Conv1d-37             [-1, 32, 2048]              96\n",
      "           Conv1d-38             [-1, 32, 2048]           1,024\n",
      "DepthwiseSeparableConv1d-39             [-1, 32, 2048]               0\n",
      "      BatchNorm1d-40             [-1, 32, 2048]              64\n",
      "           Conv1d-41            [-1, 128, 2048]           4,096\n",
      "      BatchNorm1d-42            [-1, 128, 2048]             256\n",
      "AdaptiveAvgPool1d-43               [-1, 128, 1]               0\n",
      "           Linear-44                    [-1, 8]           1,024\n",
      "             ReLU-45                    [-1, 8]               0\n",
      "           Linear-46                  [-1, 128]           1,024\n",
      "          Sigmoid-47                  [-1, 128]               0\n",
      "          SEBlock-48            [-1, 128, 2048]               0\n",
      "     Bottleneck1D-49            [-1, 128, 2048]               0\n",
      "           Conv1d-50             [-1, 64, 2048]           8,192\n",
      "      BatchNorm1d-51             [-1, 64, 2048]             128\n",
      "           Conv1d-52             [-1, 64, 1024]             192\n",
      "           Conv1d-53             [-1, 64, 1024]           4,096\n",
      "DepthwiseSeparableConv1d-54             [-1, 64, 1024]               0\n",
      "      BatchNorm1d-55             [-1, 64, 1024]             128\n",
      "           Conv1d-56            [-1, 256, 1024]          16,384\n",
      "      BatchNorm1d-57            [-1, 256, 1024]             512\n",
      "AdaptiveAvgPool1d-58               [-1, 256, 1]               0\n",
      "           Linear-59                   [-1, 16]           4,096\n",
      "             ReLU-60                   [-1, 16]               0\n",
      "           Linear-61                  [-1, 256]           4,096\n",
      "          Sigmoid-62                  [-1, 256]               0\n",
      "          SEBlock-63            [-1, 256, 1024]               0\n",
      "           Conv1d-64            [-1, 256, 1024]          32,768\n",
      "      BatchNorm1d-65            [-1, 256, 1024]             512\n",
      "     Bottleneck1D-66            [-1, 256, 1024]               0\n",
      "           Conv1d-67             [-1, 64, 1024]          16,384\n",
      "      BatchNorm1d-68             [-1, 64, 1024]             128\n",
      "           Conv1d-69             [-1, 64, 1024]             192\n",
      "           Conv1d-70             [-1, 64, 1024]           4,096\n",
      "DepthwiseSeparableConv1d-71             [-1, 64, 1024]               0\n",
      "      BatchNorm1d-72             [-1, 64, 1024]             128\n",
      "           Conv1d-73            [-1, 256, 1024]          16,384\n",
      "      BatchNorm1d-74            [-1, 256, 1024]             512\n",
      "AdaptiveAvgPool1d-75               [-1, 256, 1]               0\n",
      "           Linear-76                   [-1, 16]           4,096\n",
      "             ReLU-77                   [-1, 16]               0\n",
      "           Linear-78                  [-1, 256]           4,096\n",
      "          Sigmoid-79                  [-1, 256]               0\n",
      "          SEBlock-80            [-1, 256, 1024]               0\n",
      "     Bottleneck1D-81            [-1, 256, 1024]               0\n",
      "           Conv1d-82             [-1, 64, 1024]          16,384\n",
      "      BatchNorm1d-83             [-1, 64, 1024]             128\n",
      "           Conv1d-84             [-1, 64, 1024]             192\n",
      "           Conv1d-85             [-1, 64, 1024]           4,096\n",
      "DepthwiseSeparableConv1d-86             [-1, 64, 1024]               0\n",
      "      BatchNorm1d-87             [-1, 64, 1024]             128\n",
      "           Conv1d-88            [-1, 256, 1024]          16,384\n",
      "      BatchNorm1d-89            [-1, 256, 1024]             512\n",
      "AdaptiveAvgPool1d-90               [-1, 256, 1]               0\n",
      "           Linear-91                   [-1, 16]           4,096\n",
      "             ReLU-92                   [-1, 16]               0\n",
      "           Linear-93                  [-1, 256]           4,096\n",
      "          Sigmoid-94                  [-1, 256]               0\n",
      "          SEBlock-95            [-1, 256, 1024]               0\n",
      "     Bottleneck1D-96            [-1, 256, 1024]               0\n",
      "           Conv1d-97             [-1, 64, 1024]          16,384\n",
      "      BatchNorm1d-98             [-1, 64, 1024]             128\n",
      "           Conv1d-99             [-1, 64, 1024]             192\n",
      "          Conv1d-100             [-1, 64, 1024]           4,096\n",
      "DepthwiseSeparableConv1d-101             [-1, 64, 1024]               0\n",
      "     BatchNorm1d-102             [-1, 64, 1024]             128\n",
      "          Conv1d-103            [-1, 256, 1024]          16,384\n",
      "     BatchNorm1d-104            [-1, 256, 1024]             512\n",
      "AdaptiveAvgPool1d-105               [-1, 256, 1]               0\n",
      "          Linear-106                   [-1, 16]           4,096\n",
      "            ReLU-107                   [-1, 16]               0\n",
      "          Linear-108                  [-1, 256]           4,096\n",
      "         Sigmoid-109                  [-1, 256]               0\n",
      "         SEBlock-110            [-1, 256, 1024]               0\n",
      "    Bottleneck1D-111            [-1, 256, 1024]               0\n",
      "          Conv1d-112            [-1, 128, 1024]          32,768\n",
      "     BatchNorm1d-113            [-1, 128, 1024]             256\n",
      "          Conv1d-114             [-1, 128, 512]             384\n",
      "          Conv1d-115             [-1, 128, 512]          16,384\n",
      "DepthwiseSeparableConv1d-116             [-1, 128, 512]               0\n",
      "     BatchNorm1d-117             [-1, 128, 512]             256\n",
      "          Conv1d-118             [-1, 512, 512]          65,536\n",
      "     BatchNorm1d-119             [-1, 512, 512]           1,024\n",
      "AdaptiveAvgPool1d-120               [-1, 512, 1]               0\n",
      "          Linear-121                   [-1, 32]          16,384\n",
      "            ReLU-122                   [-1, 32]               0\n",
      "          Linear-123                  [-1, 512]          16,384\n",
      "         Sigmoid-124                  [-1, 512]               0\n",
      "         SEBlock-125             [-1, 512, 512]               0\n",
      "          Conv1d-126             [-1, 512, 512]         131,072\n",
      "     BatchNorm1d-127             [-1, 512, 512]           1,024\n",
      "    Bottleneck1D-128             [-1, 512, 512]               0\n",
      "          Conv1d-129             [-1, 128, 512]          65,536\n",
      "     BatchNorm1d-130             [-1, 128, 512]             256\n",
      "          Conv1d-131             [-1, 128, 512]             384\n",
      "          Conv1d-132             [-1, 128, 512]          16,384\n",
      "DepthwiseSeparableConv1d-133             [-1, 128, 512]               0\n",
      "     BatchNorm1d-134             [-1, 128, 512]             256\n",
      "          Conv1d-135             [-1, 512, 512]          65,536\n",
      "     BatchNorm1d-136             [-1, 512, 512]           1,024\n",
      "AdaptiveAvgPool1d-137               [-1, 512, 1]               0\n",
      "          Linear-138                   [-1, 32]          16,384\n",
      "            ReLU-139                   [-1, 32]               0\n",
      "          Linear-140                  [-1, 512]          16,384\n",
      "         Sigmoid-141                  [-1, 512]               0\n",
      "         SEBlock-142             [-1, 512, 512]               0\n",
      "    Bottleneck1D-143             [-1, 512, 512]               0\n",
      "          Conv1d-144             [-1, 128, 512]          65,536\n",
      "     BatchNorm1d-145             [-1, 128, 512]             256\n",
      "          Conv1d-146             [-1, 128, 512]             384\n",
      "          Conv1d-147             [-1, 128, 512]          16,384\n",
      "DepthwiseSeparableConv1d-148             [-1, 128, 512]               0\n",
      "     BatchNorm1d-149             [-1, 128, 512]             256\n",
      "          Conv1d-150             [-1, 512, 512]          65,536\n",
      "     BatchNorm1d-151             [-1, 512, 512]           1,024\n",
      "AdaptiveAvgPool1d-152               [-1, 512, 1]               0\n",
      "          Linear-153                   [-1, 32]          16,384\n",
      "            ReLU-154                   [-1, 32]               0\n",
      "          Linear-155                  [-1, 512]          16,384\n",
      "         Sigmoid-156                  [-1, 512]               0\n",
      "         SEBlock-157             [-1, 512, 512]               0\n",
      "    Bottleneck1D-158             [-1, 512, 512]               0\n",
      "          Conv1d-159             [-1, 128, 512]          65,536\n",
      "     BatchNorm1d-160             [-1, 128, 512]             256\n",
      "          Conv1d-161             [-1, 128, 512]             384\n",
      "          Conv1d-162             [-1, 128, 512]          16,384\n",
      "DepthwiseSeparableConv1d-163             [-1, 128, 512]               0\n",
      "     BatchNorm1d-164             [-1, 128, 512]             256\n",
      "          Conv1d-165             [-1, 512, 512]          65,536\n",
      "     BatchNorm1d-166             [-1, 512, 512]           1,024\n",
      "AdaptiveAvgPool1d-167               [-1, 512, 1]               0\n",
      "          Linear-168                   [-1, 32]          16,384\n",
      "            ReLU-169                   [-1, 32]               0\n",
      "          Linear-170                  [-1, 512]          16,384\n",
      "         Sigmoid-171                  [-1, 512]               0\n",
      "         SEBlock-172             [-1, 512, 512]               0\n",
      "    Bottleneck1D-173             [-1, 512, 512]               0\n",
      "          Conv1d-174             [-1, 256, 512]         131,072\n",
      "     BatchNorm1d-175             [-1, 256, 512]             512\n",
      "          Conv1d-176             [-1, 256, 256]             768\n",
      "          Conv1d-177             [-1, 256, 256]          65,536\n",
      "DepthwiseSeparableConv1d-178             [-1, 256, 256]               0\n",
      "     BatchNorm1d-179             [-1, 256, 256]             512\n",
      "          Conv1d-180            [-1, 1024, 256]         262,144\n",
      "     BatchNorm1d-181            [-1, 1024, 256]           2,048\n",
      "AdaptiveAvgPool1d-182              [-1, 1024, 1]               0\n",
      "          Linear-183                   [-1, 64]          65,536\n",
      "            ReLU-184                   [-1, 64]               0\n",
      "          Linear-185                 [-1, 1024]          65,536\n",
      "         Sigmoid-186                 [-1, 1024]               0\n",
      "         SEBlock-187            [-1, 1024, 256]               0\n",
      "          Conv1d-188            [-1, 1024, 256]         524,288\n",
      "     BatchNorm1d-189            [-1, 1024, 256]           2,048\n",
      "    Bottleneck1D-190            [-1, 1024, 256]               0\n",
      "          Conv1d-191             [-1, 256, 256]         262,144\n",
      "     BatchNorm1d-192             [-1, 256, 256]             512\n",
      "          Conv1d-193             [-1, 256, 256]             768\n",
      "          Conv1d-194             [-1, 256, 256]          65,536\n",
      "DepthwiseSeparableConv1d-195             [-1, 256, 256]               0\n",
      "     BatchNorm1d-196             [-1, 256, 256]             512\n",
      "          Conv1d-197            [-1, 1024, 256]         262,144\n",
      "     BatchNorm1d-198            [-1, 1024, 256]           2,048\n",
      "AdaptiveAvgPool1d-199              [-1, 1024, 1]               0\n",
      "          Linear-200                   [-1, 64]          65,536\n",
      "            ReLU-201                   [-1, 64]               0\n",
      "          Linear-202                 [-1, 1024]          65,536\n",
      "         Sigmoid-203                 [-1, 1024]               0\n",
      "         SEBlock-204            [-1, 1024, 256]               0\n",
      "    Bottleneck1D-205            [-1, 1024, 256]               0\n",
      "          Conv1d-206             [-1, 256, 256]         262,144\n",
      "     BatchNorm1d-207             [-1, 256, 256]             512\n",
      "          Conv1d-208             [-1, 256, 256]             768\n",
      "          Conv1d-209             [-1, 256, 256]          65,536\n",
      "DepthwiseSeparableConv1d-210             [-1, 256, 256]               0\n",
      "     BatchNorm1d-211             [-1, 256, 256]             512\n",
      "          Conv1d-212            [-1, 1024, 256]         262,144\n",
      "     BatchNorm1d-213            [-1, 1024, 256]           2,048\n",
      "AdaptiveAvgPool1d-214              [-1, 1024, 1]               0\n",
      "          Linear-215                   [-1, 64]          65,536\n",
      "            ReLU-216                   [-1, 64]               0\n",
      "          Linear-217                 [-1, 1024]          65,536\n",
      "         Sigmoid-218                 [-1, 1024]               0\n",
      "         SEBlock-219            [-1, 1024, 256]               0\n",
      "    Bottleneck1D-220            [-1, 1024, 256]               0\n",
      "          Linear-221                    [-1, 3]           3,075\n",
      "================================================================\n",
      "Total params: 3,646,627\n",
      "Trainable params: 3,646,627\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 174.16\n",
      "Params size (MB): 13.91\n",
      "Estimated Total Size (MB): 188.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 20\n",
    "\n",
    "# Assuming you have a ModelTrainer class defined that can handle the training process\n",
    "trainer = ModelTrainer(model, train_dataloader, val_dataloader, criterion, optimizer, device)\n",
    "\n",
    "summary(model, input_size=input_size, device=str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1defdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:34<00:00,  9.79it/s, loss=0.2690, accuracy=89.19%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.2690, Acc 89.19%, Val Loss 0.2305, Val Acc 91.00%\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1725, accuracy=93.01%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.1725, Acc 93.01%, Val Loss 0.1525, Val Acc 93.86%\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1529, accuracy=93.75%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 0.1529, Acc 93.75%, Val Loss 0.1438, Val Acc 94.24%\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.77it/s, loss=0.1428, accuracy=94.20%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 0.1428, Acc 94.20%, Val Loss 0.1485, Val Acc 93.80%\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:34<00:00,  9.79it/s, loss=0.1335, accuracy=94.50%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 0.1335, Acc 94.50%, Val Loss 0.1760, Val Acc 92.64%\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1272, accuracy=94.77%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 0.1272, Acc 94.77%, Val Loss 0.1251, Val Acc 94.79%\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1222, accuracy=95.02%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.1221, Acc 95.02%, Val Loss 0.1483, Val Acc 93.83%\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1171, accuracy=95.18%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.1171, Acc 95.18%, Val Loss 0.1164, Val Acc 95.25%\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.77it/s, loss=0.1126, accuracy=95.30%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.1126, Acc 95.30%, Val Loss 0.1111, Val Acc 95.61%\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.77it/s, loss=0.1081, accuracy=95.53%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.1081, Acc 95.53%, Val Loss 0.1204, Val Acc 95.01%\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1066, accuracy=95.55%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.1066, Acc 95.55%, Val Loss 0.1334, Val Acc 94.31%\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:35<00:00,  9.78it/s, loss=0.1038, accuracy=95.69%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.1038, Acc 95.69%, Val Loss 0.1102, Val Acc 95.40%\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:34<00:00,  9.79it/s, loss=0.1023, accuracy=95.74%]\n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.1023, Acc 95.74%, Val Loss 0.1148, Val Acc 95.36%\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [09:34<00:00,  9.79it/s, loss=0.0982, accuracy=95.94%]\n",
      "                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.0982, Acc 95.94%, Val Loss 0.1161, Val Acc 95.08%\n",
      "Stopping early due to no improvement in validation accuracy.\n",
      "Loaded best model from best_model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "trainer.run_training_loop(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9f859-d71b-4c78-95a0-3587c1e4db30",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Call and Train Model\n",
    "Loading the model structure to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d82bf",
   "metadata": {},
   "source": [
    "----\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ce3b48",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/torchsig/gradiant/Train_evaluate.py:144\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    142\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start timing for performance\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    147\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Compute model outputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(trainer.model, trainer.val_dataloader, trainer.criterion, trainer.device)\n",
    "\n",
    "labels = results['all_labels']\n",
    "predictions = results['all_predictions']\n",
    "plot_metrics(trainer, labels, predictions)\n",
    "\n",
    "plot_conf_matrix( labels, predictions,selected_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0124692",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb2619d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/family_im_new_m_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/serialization.py:665\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/storage.py:121\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.save(trainer.model, './models/family_im_new_m_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d659f053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNet1D(Bottleneck1D, [3,4,4,3], num_classes=len(selected_classes), in_channels=2).to(device)\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "state_dict = torch.load('./models/PSK_im_new_m')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57661ddc",
   "metadata": {},
   "source": [
    "### Accuracy based on SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc5bba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snr_bin(snr):\n",
    "    if snr <= 5:\n",
    "        return '-2 to 5'\n",
    "    elif 6 <= snr <= 10:\n",
    "        return '6 to 10'\n",
    "    elif 11 <= snr <= 15:\n",
    "        return '11 to 15'\n",
    "    elif 16 <= snr <= 20:\n",
    "        return '16 to 20'\n",
    "    elif 21 <= snr <= 25:\n",
    "        return '21 to 25'\n",
    "    elif 26 <= snr <= 30:\n",
    "        return '26 to 30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4b2dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig53_metadata = Sig53(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    impaired=impaired,\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a53404ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR Bin -2 to 5: Accuracy = 40.41% (Count = 2316)\n",
      "SNR Bin 6 to 10: Accuracy = 51.58% (Count = 1140)\n",
      "SNR Bin 11 to 15: Accuracy = 69.57% (Count = 1288)\n",
      "SNR Bin 16 to 20: Accuracy = 79.31% (Count = 1252)\n",
      "SNR Bin 21 to 25: Accuracy = 86.71% (Count = 1204)\n",
      "SNR Bin 26 to 30: Accuracy = 88.64% (Count = 1312)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# Evaluation per SNR bin\n",
    "model.eval()\n",
    "model.to(device)\n",
    "snr_bins = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "        # Get corresponding SNR values from the metadata\n",
    "        start_idx = batch_idx * 16\n",
    "        end_idx = start_idx + 16\n",
    "        snrs = [sig53_metadata[idx][1][1] for idx in range(start_idx, end_idx)]\n",
    "\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for label, pred, snr in zip(labels, predicted, snrs):\n",
    "            bin_name = get_snr_bin(snr)\n",
    "            snr_bins[bin_name]['total'] += 1\n",
    "            if label == pred:\n",
    "                snr_bins[bin_name]['correct'] += 1\n",
    "\n",
    "# Define the bin order\n",
    "bin_order = ['-2 to 5', '6 to 10', '11 to 15', '16 to 20', '21 to 25', '26 to 30']\n",
    "\n",
    "# Calculate and print accuracy per SNR bin in order\n",
    "for bin_name in bin_order:\n",
    "    if bin_name in snr_bins:\n",
    "        stats = snr_bins[bin_name]\n",
    "        accuracy = 100 * stats['correct'] / stats['total']\n",
    "        count = stats['total']\n",
    "        print(f\"SNR Bin {bin_name}: Accuracy = {accuracy:.2f}% (Count = {count})\")\n",
    "    else:\n",
    "        print(f\"SNR Bin {bin_name}: No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0eb33a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mselected_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Check if CUDA is available and set the device accordingly\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/torchsig/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNet1D(Bottleneck1D, [4,5,5,4], num_classes=len(selected_classes), in_channels=2).to(device)\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2ac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
